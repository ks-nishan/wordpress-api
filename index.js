import WPAPI from "wpapi";

console.log("Starting...");

// You must authenticate to be able to POST (create) a post
var wp = new WPAPI({
  endpoint: "http://localhost:10010/wp-json",
  username: "nishakanaga0708@gmail.com",
  password: "E3PA PE9Y GZRv 8iID 3nV2 GRet", // application password which is generated from WP admin
});

// Function to create a post
async function createWordpressPost() {
  try {
    console.log("creating demo post...");

    let BeforeContent = "<p><img src=\"/uploads/dc64a919-1200-42d3-90c6-9bc96dd9f4fa/_c3LSDcMKy.png\">Constructing a comprehensive solution tailored for computer vision using Machine Learning Operations (MLOps) is often demanding, and requires a strategic integration of open-source tools.</p><p>Therefore, this guide will walk you through the key stages of building a robust MLOps pipeline, from data collection to model deployment, using popular open-source software (OSS) platforms.</p><h2>MLOps Lifecycle</h2><p>The MLOps lifecycle consists of seven iterative stages, as shown in the graphic below, which is crucial for the success of a machine learning system.</p><blockquote>Each stage requires careful execution, and setbacks may necessitate revisiting previous steps to address issues.</blockquote><p>Here's a breakdown of each stage:</p><h1><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/GLWXy2sHCX.png\">Data Collection &amp; Management</h1><h2>Leveraging Min.IO</h2><p>Efficient data collection sets the foundation for any successful computer vision model. Leveraging Min.IO an open-source platform that ensures scalable and reliable storage solutions. With data being the cornerstone of any machine learning project, organizing and accessing it seamlessly is critical for effective model development.</p><p><a href=\"https://min.io/product/overview\" rel=\"noopener noreferrer\" target=\"_blank\"><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/hcws9y49Kr.png\"></a></p><p><strong><span>Key features of MinIO</span></strong></p><ul><li><span>Compatibility with S3 applications, high throughput, and low latency. </span></li><li><span>MinIO can be easily deployed on various platforms, including on-premises hardware or in the cloud. </span></li><li><span>It is suitable for a wide range of use cases, such as data lake storage, backup and recovery, and content delivery. </span></li><li><span>It provides robust security features, including encryption and access control, making it a versatile solution for organizations with diverse storage needs.</span></li></ul><p>To understand how MinIO can be used for computer vision projects, let's break down key concepts such as Object Storage, Buckets, and S3 in the context of MinIO with a traffic sign detection example.</p><p><a href=\"https://min.io/product/overview\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/J9zzHM979T.png\"></strong></a><strong>Object Storage: </strong>In MinIO, data is organized as objects, where each object typically represents a file or a piece of data. Objects can be of any type, such as images, videos, documents, etc.</p><p><strong>Buckets: </strong>Buckets help in organizing and managing data, acting as a top-level namespace for objects. You can create multiple buckets within a MinIO instance, and each bucket has its own unique namespace.</p><p><strong>S3 Compatibility</strong>: MinIO is compatible with S3 API, which is the standard interface for interacting with object storage in the AWS ecosystem. S3-compatible services, like MinIO, allow you to use existing S3 tools and libraries, making it easy to integrate MinIO into applications that were originally built for AWS S3.</p><blockquote><a href=\"https://dagshub.com/docs/integration_guide/set_up_remote_storage_for_data_and_models/#:~:text=DagsHub%20supports%20connecting%20external%20storage,without%20leaving%20the%20DagsHub%20platform.\" rel=\"noopener noreferrer\" target=\"_blank\">DagsHub also supports connecting AWS S3, GCS, and </a><a href=\"https://dagshub.com/docs/integration_guide/set_up_remote_storage_for_data_and_models/#:~:text=DagsHub%20supports%20connecting%20external%20storage,without%20leaving%20the%20DagsHub%20platform.\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: rgb(255, 187, 42);\"><span>MinIO</span></a><a href=\"https://dagshub.com/docs/integration_guide/set_up_remote_storage_for_data_and_models/#:~:text=DagsHub%20supports%20connecting%20external%20storage,without%20leaving%20the%20DagsHub%20platform.\" rel=\"noopener noreferrer\" target=\"_blank\"> to Dagshub Repos</a></blockquote><p><strong>Organizing Data with Buckets</strong>:</p><p>Buckets help you organize your object detection data. For example, you might create different buckets for different projects, teams, or types of datasets. Within each bucket, you can store related object detection data, making it easier to manage and retrieve.</p><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/nsU9GfIQWQ.png\">I have created a MinIO storage bucket to store our traffic sign detection dataset, which includes images, annotations, configuration files, and other metadata. Each annotated image or dataset file is treated as a separate object within a bucket.</p><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/REeZrny5g2.png\"></p><blockquote>Follow the <a href=\"https://min.io/docs/minio/linux/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">MinIO documentation</a> for configuring the MinIO server and client. The <span>MinIO console gives you a quick overview of the overall state of your MinIO deployment.</span></blockquote><p><strong><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/hHf41O2i2C.png\"></strong></p><h1>Data Labeling</h1><h2>Harnessing the Power of Label Studio</h2><p><span>Accurate labelling of data is critical for training reliable computer vision models.&nbsp;</span><a href=\"https://labelstud.io/guide/install\" rel=\"noopener noreferrer\" target=\"_blank\"><span>Label Studio</span></a><span>, an open-source data labelling tool, provides a versatile and efficient solution for annotating images, ensuring the high-quality labelled datasets necessary for training precise models.</span></p><p><strong>Key features of Label Studio include:</strong></p><p><strong>Versatility</strong>: Supporting multiple annotation tasks such as image classification, object detection, named entity recognition, and more. .</p><p><strong>Collaboration: </strong>It facilitates collaboration among teams by providing a user-friendly interface for annotators and supporting multiple annotator roles.</p><p><strong>Integration: </strong>Label Studio can be integrated with different machine learning frameworks and libraries, allowing users to seamlessly incorporate labelled data into their training pipelines. It supports popular formats for exporting labelled datasets.</p><blockquote>Label Studio supports integration with Amazon S3 for storing and managing labelled data.</blockquote><p><strong>Ease of Use: </strong>The platform is designed to be user-friendly, making it accessible to both technical and non-technical users.</p><p><strong>Customization:</strong> Users can create custom annotation interfaces tailored to their specific use cases.</p><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/xxXvIY1sl4.png\"></p><p>Here, I have used Label Studio to make annotations to our traffic sign dataset.</p><blockquote>You can use DagsHub to<span> </span><a href=\"https://dagshub.com/blog/label-studio-crash-course/\" rel=\"noopener noreferrer\" target=\"_blank\"><span>automate your labeling and annotation process</span></a><span> with Label Studio.</span></blockquote><h1>Data Visualization and Curation</h1><h2><span>Voxel51 for Enhanced Insights</span></h2><p>Voxel51 offers a powerful MLOps framework for data visualization and curation. With its comprehensive suite of tools, you can explore and curate your datasets, gaining insights into the characteristics of your data that are crucial for developing effective computer vision models.</p><p><strong><span>Features of Voxel51:</span></strong></p><ul><li><span>Dataset curation </span></li><li><span>Model evaluation </span></li><li><span>Error identification</span></li><li><span>Embedding visualization</span></li><li><span>Expediting the transition to production, ultimately aiding teams in achieving better model performance.</span></li></ul><h1><img src=\"https://www.writergate.com/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/zCNgmeokU4.png\"></h1><p>Voxel51 provides relatively many functions and plugins for data <span>deduplication</span>, data sampling, deleting samples with respective tags, filtering data, dimensionality reductions, correcting annotations with errors, <span>building data-centric pipelines, and integrations </span>with your favorite ML tools like Colab, Label Studio, Jupyter, TensorFlow, Google Cloud, AWS, etc.</p><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/s736qsezJ4.png\"></p><p><span>In the Voxel51 dashboard, we have the capability to visualize our traffic sign dataset along with annotations. Leveraging the platform's plugins and functions, we can actively engage in refining our data to create an unbiased and balanced dataset. </span></p><p><span>This process is instrumental in preparing the dataset for optimal performance during model training, testing, and validation stages. The interactive features provided by Voxel51 empower us to make informed adjustments, ensuring the dataset is well-curated and aligned with the specific requirements of our computer vision model.</span></p><h1><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/Lbk8jKbuNo.png\">Model &amp; Experiment Tracking</h1><h2>Using MLFlow for Development</h2><p><span>MLflow simplifies the management of machine learning experiments. With MLflow, you can track and compare experiments, log parameters, and store model artifacts. This open-source platform ensures reproducibility and accountability throughout the model development process.</span></p><p><a href=\"https://mlflow.org/docs/latest/getting-started/index.html\" rel=\"noopener noreferrer\" target=\"_blank\"><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/B8pMnTg-u3.png\"></a>The <a href=\"https://mlflow.org/docs/latest/models.html\" rel=\"noopener noreferrer\" target=\"_blank\"><span>MLflow documentation</span></a> on models provides the functionality related to managing and deploying machine learning models within the MLflow platform.</p><p><strong>Key Features</strong>:</p><ol><li><strong>Model Packaging:</strong> MLflow supports packaging machine learning models in a standardized format, allowing for easy distribution and sharing.</li><li><strong>Model Formats:</strong> MLflow is versatile and supports various machine learning frameworks and libraries, enabling users to work with models developed using different technologies.</li><li><strong>Model Registration:</strong> MLflow provides a model registry to keep track of different versions of models, making it easier to organize, document, and reproduce experiments.</li><li><strong>Model Deployment:</strong> MLflow facilitates the deployment of machine learning models to various deployment environments, including cloud platforms and containerized environments, ensuring a smooth transition from development to production.</li><li><strong>Tracking Models:</strong> MLflow allows users to log and track experiments, parameters, and metrics associated with model development, aiding in reproducibility and collaboration.</li></ol><p>You can create an integration with <a href=\"https://dagshub.com/docs/integration_guide/mlflow_tracking/#:~:text=DagsHub%20provides%20a%20free%20hosted,built%20into%20your%20DagsHub%20project.\" rel=\"noopener noreferrer\" target=\"_blank\">DagsHub and MLflow</a> as DagsHub provides a free hosted MLflow server with team-based access control for every repository.</p><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/_lyMUYziql.png\">You can authorize GitHub to integrate your model repository into DagsHub, and as shown in the above image, you can host your repository for experimentation and tracking on MLflow.</p><h1><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/x_agbZ-Jyq.png\">Model Deployment</h1><h2>BentoML for Seamless Integration</h2><p>In the ML pipeline, model deployment is a crucial stage where open-source tools like BentoML simplify the deployment of models, allowing you to turn your trained models into scalable and production-ready APIs. Its modular and flexible design ensures seamless integration with various deployment environments, making it an ideal choice for computer vision applications.</p><p>Our trained and developed Object detection model can now be deployed using BentoML. There are a few steps that you need to follow to get the model deployed.</p><p><strong>1. Save our trained model into local storage and load it back to BentoML.</strong></p><pre class=\"ql-syntax\" spellcheck=\"false\">model.save(\"C:/Users/your_path/Downloads/archive/assets/CNN.keras\")\n</pre><p><strong>Install dependencies</strong>:</p><pre class=\"ql-syntax\" spellcheck=\"false\">pip install bentoml\npip install tensorflow\npip install requests\n</pre><p>After we save our trained model locally, load and save our model to BentoML.</p><pre class=\"ql-syntax\" spellcheck=\"false\">from pathlib import Path\nfrom tensorflow import keras\nimport bentoml\n\ndef load_model_and_save_to_bentoML(model_file: Path) -&gt; None:\n    model = keras.models.load_model(model_file)\n    bento_model = bentoml.keras.save_model(\"keras_model\", model)\n    print(f\"Bento model tag = {bento_model.tag}\")\n    \nload_model_and_save_to_bentoML(Path(\"C:/Users/minur/Downloads/archive/assets/CNN.keras\"))\n</pre><p>Run this code on your command line to check the list of BentoML models saved.</p><pre class=\"ql-syntax\" spellcheck=\"false\">bentoml models list\n</pre><p>Create an endpoint to access Bento model and use the script to make predictions using BentoML.</p><pre class=\"ql-syntax\" spellcheck=\"false\">import numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\nBENTO_MODEL_TAG = \"keras_model:ze7wq5oh3g6nm5b2\"\n\n\nrecognition_runner = bentoml.keras.get(BENTO_MODEL_TAG).to_runner()\n\ntraffic_recognition_service = bentoml.Service(\"traffic_sign_recognition\", runners=[recognition_runner])\n\n\n@traffic_recognition_service.api(input=NumpyNdarray(), output=NumpyNdarray()) # use input outputs appropriate to your data\ndef recognize(input_data: np.ndarray()) -&gt; np.ndarray():\n    return recognition_runner.predict.run(input_data)\n</pre><p>Run the following service script to get predictions by parsing the test dataset to our model using a service request to the bentoml server.</p><blockquote>Note: Use <code><em>bentoml serve service.py</em></code> on the command line to run the script.</blockquote><pre class=\"ql-syntax\" spellcheck=\"false\">from typing import Tuple\nimport json\n\nimport numpy as np\nimport requests\n# import test dataset from model training folder from the local drive\nfrom achieve.train import test_traffic_sign_images  \n\n\nSERVICE_URL = \"http://localhost:3000/traffic-signs\"\n\n\ndef sample_random_recognition_data_point() -&gt; Tuple[np.ndarray, np.ndarray]:\n    _, _, test_images, test_labels = test_traffic_sign_images() # Prepare your data to return images and labels\n    random_index = np.random.randint(0, len(test_images))\n    random_test_image = test_images[random_index]\n    random_test_image = np.expand_dims(random_test_image, 0)\n    return random_test_image, test_labels[random_index]\n\n\ndef make_request_to_bento_service(\n    service_url: str, input_array: np.ndarray\n) -&gt; str:\n    serialized_input_data = json.dumps(input_array.tolist())\n    response = requests.post(\n        service_url,\n        data=serialized_input_data,\n        headers={\"content-type\": \"application/json\"}\n    )\n    return response.text\n\n\ndef main():\n    input_data, expected_output = sample_random_recognition_data_point()\n    prediction = make_request_to_bento_service(SERVICE_URL, input_data)\n    print(f\"Prediction: {prediction}\")\n    print(f\"Expected output: {expected_output}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</pre><p><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/dxgnoJBTlv.png\"></p><p><strong>2. Create a BentoML Service</strong></p><p>You can build your BentoML Service by going through their <a href=\"https://docs.bentoml.org/en/latest/guides/services.html\" rel=\"noopener noreferrer\" target=\"_blank\">documentation</a>. To keep things simple in this article, I won't be diving deep into BentoML, but feel free to get the sample code from their documentation.</p><p><strong>3. Build Model + Service Into the BentoML</strong></p><p>After you've built your service, you'd have to build your model and service into a Bento. To do so, check out their documentation for a clearer <a href=\"https://docs.bentoml.org/en/latest/guides/build-options.html\" rel=\"noopener noreferrer\" target=\"_blank\">implementation</a>. Afterward, you can directly server your model through BentoML</p><p><strong>4. Dockerise Bento &amp; Run BentoML Service via Docker</strong></p><p>Finally, you can <a href=\"https://docs.bentoml.org/en/latest/guides/containerization.html\" rel=\"noopener noreferrer\" target=\"_blank\">distribute your Model as a packaged application using container based platform such as Docker</a>.</p><h1>Model Monitoring and Error Analysis</h1><p><span>Finally, after you've deployed your model, you must start monitoring it. This ensures that your model is consistently looked at and issues are detected early on. </span></p><p>To do so, you can leverage <a href=\"https://www.evidentlyai.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><span>EvidentlyAI</span></a><span>, an open-source library, that provides tools for model monitoring and error analysis. </span></p><blockquote>Note, EvidentlyAI is not specifically designed for computer vision models, however, <a href=\"https://alimbekov.com/en/evidently-and-custom-metrics/\" rel=\"noopener noreferrer\" target=\"_blank\">it can be adopted</a> to monitor computer vision models.</blockquote><p><span>With its capabilities in statistical analysis and visualization, you can proactively identify deviations and ensure your computer vision model stays at its peak performance.</span></p><h1><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/8QnJW0Z_T9.png\"></h1><p>You can integrate your hosted models on MLflow with EvidentlyAI to automate model quality evaluations.</p><h1><img src=\"/uploads/5c63dd4d-eaa4-45e2-88c7-812fb804d415/uoAPyq-ZsY.png\">Conclusion</h1><p><span>This guide outlines the construction of a comprehensive MLOps pipeline for computer vision using open-source tools, covering key stages from data collection to model deployment. Leveraging tools such as Min.IO, Label Studio, Voxel51, MLflow, BentoML, and EvidentlyAI, the guide emphasizes the importance of strategic integration for an effective workflow. </span></p><p><a href=\"https://dagshub.com/about\" rel=\"noopener noreferrer\" target=\"_blank\"><span>Dagshub </span></a><span>acts as a unifying platform, seamlessly integrating with these open-source tools to provide a cohesive solution for managing the end-to-end MLOps lifecycle in collaborative environments.</span></p>"

    const modifiedContent = BeforeContent.replace(/"\/uploads/g, '"https://www.writergate.com/uploads');

    const response = await wp.posts().create({
      title: "Full Article demo 09",
      content: modifiedContent,
      status: "draft",
    });
    console.log("Post created successfully. Post :", response);
  } catch (error) {
    console.error("Error creating post:", error);
  }
}

// Call the function to create the demo post
createWordpressPost();

async function toggelWordpressPost(postId) {
  try {
    console.log("toggeling the post...");
    const post = await wp.posts().id(postId);

    let newStatus = post.status === "draft" ? "publish" : "draft";
    const response = await wp.posts().id(postId).update({
      status: newStatus,
    });
    console.log("Post updated successfully :", response.author);
  } catch (error) {
    console.error("Error creating post:", error);
  }
}

// call the function to update the status
// toggelWordpressPost(24);

async function getAllAuthors() {
  try {
    const responce = await wp.users();
    console.log("All Authors : ", responce);
  } catch (error) {
    console.log("Error while getting authors : ", error);
  }
}
// getAllAuthors();

async function getAllPublishedPosts() {
  try {
    const responce = await wp.posts();
    console.log("All Posts : ", responce);
  } catch (error) {
    console.log("Error while getting Posts : ", error);
  }
}
// getAllPublishedPosts();

async function getAuthor(postId) {
  try {
    const post = await wp.posts().id(postId);
    const author = await wp.users().id(post.author);

    console.log('Author Details : ', author);
  } catch (error) {
    console.log('Error while get author info : ', error);
  }
}
// getAuthor(36);
